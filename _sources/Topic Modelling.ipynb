{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1U3SZ3Xfy63neG1DQ7D1VjeWeL5uoTIfG","authorship_tag":"ABX9TyOS2iTYKTvaG/2/2kGZnOKO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Topic Modelling"],"metadata":{"id":"w_bPWUtO1ZVw"}},{"cell_type":"markdown","source":["## Crawling data From Youtube"],"metadata":{"id":"D5h8i2_G1eVi"}},{"cell_type":"markdown","source":["**Tujuan** dari program ini adalah melakukan crawling (pengambilan) data komentar pada sebuah video Youtube menggunakan **Youtube Data API v3**. Sebelum mencoba program ini, pastikan Anda sudah memiliki (mengaktifkan) layanan Youtube Data API dan telah membangkitkan **API Key**. \n","\n","Jika belum memiliki **API KEY**, Anda dapat mengikuti petunjuk singkat sebagai berikut: \n","1. Login ke Google Developer Console (https://console.developers.google.com/)dengan akun Google Anda\n","2. Buat project baru dan lengkapi isian yang diminta. \n","3. Aktifkan Layanan API pada halaman project, dan cari **Youtube Data API v3**.\n","4. Dari halaman dashboard, buat kredential agar API tersebut dapat digunakan. Klik tombol **Buat Kredensial** (**Create Credential**). Lengkapi isian formnya.\n","5. Anda dapat mengakses / melihat API KEY pada tab **Credentials**.\n","\n"],"metadata":{"id":"xFOlKzgY1vla"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"W_VDqUi8tIMW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%cd /content/drive/My Drive/prosaindata/"],"metadata":{"id":"6nWNgvmRtL6J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#install library\n","!pip install sastrawi\n","!pip install swifter\n","!pip install gensim"],"metadata":{"id":"NLiIc65GtgMR"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MxC4ruVF1E4H"},"outputs":[],"source":["#import library\n","import pandas as pd\n","from googleapiclient.discovery import build\n","import numpy as np\n","from string import punctuation\n","import re\n","import nltk"]},{"cell_type":"code","source":["#Membuat function untuk crawling data\n","def video_comments(video_id):\n","\t# empty list for storing reply\n","\treplies = []\n","\n","\t# creating youtube resource object\n","\tyoutube = build('youtube', 'v3', developerKey=api_key)\n","\n","\t# retrieve youtube video results\n","\tvideo_response = youtube.commentThreads().list(part='snippet,replies', videoId=video_id).execute()\n","\n","\t# iterate video response\n","\twhile video_response:\n","\t\t\n","\t\t# extracting required info\n","\t\t# from each result object\n","\t\tfor item in video_response['items']:\n","\t\t\t\n","\t\t\t# Extracting comments ()\n","\t\t\tpublished = item['snippet']['topLevelComment']['snippet']['publishedAt']\n","\t\t\tuser = item['snippet']['topLevelComment']['snippet']['authorDisplayName']\n","\n","\t\t\t# Extracting comments\n","\t\t\tcomment = item['snippet']['topLevelComment']['snippet']['textDisplay']\n","\t\t\tlikeCount = item['snippet']['topLevelComment']['snippet']['likeCount']\n","\n","\t\t\treplies.append([published, user, comment, likeCount])\n","\t\t\t\n","\t\t\t# counting number of reply of comment\n","\t\t\treplycount = item['snippet']['totalReplyCount']\n","\n","\t\t\t# if reply is there\n","\t\t\tif replycount>0:\n","\t\t\t\t# iterate through all reply\n","\t\t\t\tfor reply in item['replies']['comments']:\n","\t\t\t\t\t\n","\t\t\t\t\t# Extract reply\n","\t\t\t\t\tpublished = reply['snippet']['publishedAt']\n","\t\t\t\t\tuser = reply['snippet']['authorDisplayName']\n","\t\t\t\t\trepl = reply['snippet']['textDisplay']\n","\t\t\t\t\tlikeCount = reply['snippet']['likeCount']\n","\t\t\t\t\t\n","\t\t\t\t\t# Store reply is list\n","\t\t\t\t\t#replies.append(reply)\n","\t\t\t\t\treplies.append([published, user, repl, likeCount])\n","\n","\t\t\t# print comment with list of reply\n","\t\t\t#print(comment, replies, end = '\\n\\n')\n","\n","\t\t\t# empty reply list\n","\t\t\t#replies = []\n","\n","\t\t# Again repeat\n","\t\tif 'nextPageToken' in video_response:\n","\t\t\tvideo_response = youtube.commentThreads().list(\n","\t\t\t\t\tpart = 'snippet,replies',\n","\t\t\t\t\tpageToken = video_response['nextPageToken'], \n","\t\t\t\t\tvideoId = video_id\n","\t\t\t\t).execute()\n","\t\telse:\n","\t\t\tbreak\n","\t#endwhile\n","\treturn replies\n"],"metadata":{"id":"y3SofpG716Dy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# isikan dengan api key Anda\n","api_key = 'AIzaSyBcQknzxNArq2ASQeN3IXu-PkvyugNKhPs'\n","\n","# Enter video id\n","# contoh url video = https://www.youtube.com/watch?v=5tucmKjOGi8\n","video_id = \"KtntKGlmuZw\" #isikan dengan kode / ID video\n","\n","# Call function\n","comments = video_comments(video_id)\n","\n","comments"],"metadata":{"id":"vxEMW2b92Q8C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#menjadikan dataframe\n","df = pd.DataFrame(comments, columns=['publishedAt', 'authorDisplayName', 'text', 'likeCount'])\n","df"],"metadata":{"id":"ZmUrZq2TlneW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%cd /content/drive/My Drive/prosaindata/"],"metadata":{"id":"ODirHpjLHb2S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#simpan hasil crawling ke csv\n","df.to_csv('youtube_comments.csv', index=False)"],"metadata":{"id":"kaB7Q7erlz1C"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Preprocessing"],"metadata":{"id":"mrvFYf66m-ob"}},{"cell_type":"markdown","source":["### 1. Symbol & Punctuation Removal, case folding\n","\n","Pada Tahap ini preprocessing yang dilakukan yaitu menghilangkan simbol dan tanda baca, serta melakukan case folding yaitu merubah seluruh huruf yang ada pada data menjadi huruf kecil"],"metadata":{"id":"aewQBoiQnDV6"}},{"cell_type":"code","source":["#proses menghilangkan simbol dan emoji\n","def remove_text_special (text):\n","  text = text.replace('\\\\t',\"\").replace('\\\\n',\"\").replace('\\\\u',\"\").replace('\\\\',\"\")\n","  text = text.encode('ascii', 'replace').decode('ascii')\n","  return text.replace(\"http://\",\" \").replace(\"https://\", \" \")\n","df['text'] = df['text'].apply(remove_text_special)\n","print(df['text'])"],"metadata":{"id":"yr3Yaw4cnAoV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#menghilangkan tanda baca\n","def remove_tanda_baca(text):\n","  text = re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",text)\n","  return text\n","\n","df['text'] = df['text'].apply(remove_tanda_baca)\n","df['text'].head(20)"],"metadata":{"id":"4lhXPWT6ngWs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#proses menghilangkan angka\n","def remove_numbers (text):\n","  return re.sub(r\"\\d+\", \"\", text)\n","df['text'] = df['text'].apply(remove_numbers)\n","df['text']"],"metadata":{"id":"gGXyodK4nt8G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#proses casefolding\n","def casefolding(Comment):\n","  Comment = Comment.lower()\n","  return Comment\n","df['text'] = df['text'].apply(casefolding)\n","df['text']"],"metadata":{"id":"dA_xNQ92oH_7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 2. Tokenizing\n","Pada tahap ini preprocessing yang dilakukan adalah tokenizing. Tokenizing adalah metode untuk melakukan pemisahan kata dalam suatu kalimat dengan tujuan untuk proses analisis teks lebih lanjut"],"metadata":{"id":"JNBfXpEJor40"}},{"cell_type":"code","source":["#proses tokenisasi\n","# from nltk.tokenize import TweetTokenizer\n","nltk.download('punkt')\n","# def word_tokenize(text):\n","#   tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n","#   return tokenizer.tokenize(text)\n","\n","df['review_token'] = df['text'].apply(lambda sentence: nltk.word_tokenize(sentence))\n","df['review_token']"],"metadata":{"id":"3LfP-S4loutU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 3. Word Normalization\n","Pada tahap ini yang dilakukan yaitu normalisasi pada data. Hal tersebut dilakukan untuk merubah kata yang tidak baku menjadi kata baku"],"metadata":{"id":"zZ1eDS45oXSD"}},{"cell_type":"code","source":["# #Normalisasi kata tidak baku\n","# normalize = pd.read_excel(\"tugas/Dataset/Normalization Data.xlsx\")\n","\n","# normalize_word_dict = {}\n","\n","# for row in normalize.iterrows():\n","#   if row[0] not in normalize_word_dict:\n","#     normalize_word_dict[row[0]] = row[1]\n","\n","# def normalized_term(comment):\n","#   return [normalize_word_dict[term] if term in normalize_word_dict else term for term in comment]\n","\n","# df['comment_normalize'] = df['review_token'].apply(normalized_term)\n","# df['comment_normalize'].head(20)"],"metadata":{"id":"C9lf6-kSoZ7V"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 4. Stopwords Removal\n","Pada tahap ini preprocessing yang dilakukan adalah menghilangkan kata yang tidak penting. Stopwords removal dilakukan 2 kali, yang pertama berdasarkan korpus yang ada di library python yaitu nltk, yang kedua berdasarkan file 'list_stopwords'"],"metadata":{"id":"rnFpCOGcozSU"}},{"cell_type":"code","source":["#Stopword Removal\n","nltk.download('stopwords')\n","from nltk.corpus import stopwords\n","txt_stopwords = stopwords.words('indonesian')\n","\n","def stopwords_removal(filtering) :\n","  filtering = [word for word in filtering if word not in txt_stopwords]\n","  return filtering\n","\n","df['stopwords_removal'] = df['comment_normalize'].apply(stopwords_removal)\n","df['stopwords_removal'].head(20)"],"metadata":{"id":"M5rs8zH9oxBM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# #stopword removal 2\n","# data_stopwords = pd.read_excel(\"tugas/Dataset/list_stopwords.xlsx\")\n","# print(data_stopwords)\n","\n","# def stopwords_removal2(filter) :\n","#   filter = [word for word in filter if word not in data_stopwords]\n","#   return filter\n","\n","# df['stopwords_removal_final'] = df['stopwords_removal'].apply(stopwords_removal2)\n","# df['stopwords_removal_final'].head(20)"],"metadata":{"id":"M9HOlEYro4EU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 5. Stemming\n","Pada tahap ini preprocessing yang dilakukan adalah stemming. Stemming adalah proses pemetaan dan penguraian bentuk dari suatu kata menjadi bentuk kata dasarnya."],"metadata":{"id":"sAm0PGY5pobg"}},{"cell_type":"code","source":["#proses stem\n","from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n","import string\n","import swifter\n","factory = StemmerFactory()\n","stemmer = factory.create_stemmer()\n","\n","def stemming (term):\n","  return stemmer.stem(term)\n","\n","term_dict = {}\n","for document in df['stopwords_removal_final']:\n","  for term in document:\n","    if term not in term_dict:\n","      term_dict[term] = ''\n"],"metadata":{"id":"cpdEUZOEpqHP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(len(term_dict))\n","print(\"-----------------------------\")"],"metadata":{"id":"TrIvVyg2qe2r"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# for term in term_dict:\n","#   term_dict[term] = stemming(term)\n","#   print(term,\":\",term_dict[term])\n","\n","# print(term_dict)\n","# print(\"-----------------------------\")"],"metadata":{"id":"b_vMweH3qgqj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_stemming(document):\n","  return [term_dict[term] for term in document]"],"metadata":{"id":"ntQmeiGwqjoT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df['stemming'] = df['stopwords_removal_final'].swifter.apply(get_stemming)"],"metadata":{"id":"1IaLcbKuqldi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(df['stemming'])"],"metadata":{"id":"tlMVynwCqnty"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.head(20)"],"metadata":{"id":"0PTsdjKOqp3r"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Feature Extraction (TF-IDF)"],"metadata":{"id":"cJsUF8Kptv6C"}},{"cell_type":"markdown","source":["Algoritma TF-IDF (Term Frequency – Inverse Document Frequency) adalah salah satu algoritma yang dapat digunakan untuk menganalisa hubungan antara sebuah frase/kalimat dengan sekumpulan dokumen. Contoh yang dibahas kali ini adalah mengenai penentuan urutan peringkat data berdasarkan query yang digunakan.\n","Inti utama dari algoritma ini adalah melakukan perhitungan nilai TF dan nilai IDF dari sebuah setiap kata kunci terhadap masing-masing dokumen"],"metadata":{"id":"jxFAlQOIGq3U"}},{"cell_type":"code","source":["def joinkata(data):\n","  kalimat = \"\"\n","  for i in data:\n","    kalimat += i\n","    kalimat += \" \"\n","  return kalimat\n","\n","text = df['stemming'].swifter.apply(joinkata)\n","text"],"metadata":{"id":"mHkWIRvutysp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Vectorize document using TF-IDF\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","vectorizer = TfidfVectorizer(lowercase=True,\n","                        stop_words='english',\n","                        ngram_range = (1,1)\n","                        )\n","\n","# Fit and Transform the documents\n","tfidf_matrix = vectorizer.fit_transform(text)"],"metadata":{"id":"FKrIfm0CwUQC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(tfidf_matrix)"],"metadata":{"id":"rfpoGUK11Vl8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Latent Semantic Analysis"],"metadata":{"id":"5PFwXitn1IMJ"}},{"cell_type":"code","source":["# Melakukan dekomposisi matriks dengan SVD\n","from sklearn.decomposition import TruncatedSVD\n","svd_model = TruncatedSVD(n_components=4)\n","lsa_matrix = svd_model.fit_transform(tfidf_matrix)"],"metadata":{"id":"9W9hfDLL1OEp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Modelling"],"metadata":{"id":"3dN1sfkPgMTy"}},{"cell_type":"markdown","source":["Bobot kata terhadap masing masing topik"],"metadata":{"id":"R0y3gp2GgPCA"}},{"cell_type":"code","source":["# bobot kata terhadap masing masing topik\n","terms = vectorizer.get_feature_names_out()\n","\n","for index, component in enumerate(svd_model.components_):\n","    zipped = zip(terms, component)\n","    top_terms_key=sorted(zipped, key = lambda t: t[1], reverse=True)[:2]\n","    print(\"Topic \"+str(index)+\": \",top_terms_key)"],"metadata":{"id":"mFIkYseZgRYY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Bobot setiap topik terhadap dokumen"],"metadata":{"id":"dLO0dcmrgUG_"}},{"cell_type":"code","source":["# bobot setiap topik terhadap  dokumen\n","df_lsa = pd.DataFrame(lsa_matrix, columns=[\"Topik 0\", \"Topik 1\", \"Topik 2\", \"Topik 3\"])\n","df_lsa = pd.concat([text, df_lsa], axis=1)\n","df_lsa['Topik']= df_lsa[['Topik 0', 'Topik 1', 'Topik 2', 'Topik 3']].apply(lambda x: x.argmax(), axis=1)\n","\n","df_lsa"],"metadata":{"id":"Go5h22S6w0hW"},"execution_count":null,"outputs":[]}]}